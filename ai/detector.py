import os
import cv2
import dlib
import numpy as np
import matplotlib.pyplot as plt
import torch
from PIL import Image
from torchvision import transforms
from imutils import face_utils
import clip

# =====================
# ëª¨ë¸ ë° ì „ì²˜ë¦¬ ì´ˆê¸°í™”
# =====================

# dlib ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
detector_path = os.path.join('models', 'dogHeadDetector.dat')
predictor_path = os.path.join('models', 'landmarkDetector.dat')

# dlib ëª¨ë¸ ë¡œë“œ
detector = dlib.cnn_face_detection_model_v1(detector_path)
predictor = dlib.shape_predictor(predictor_path)

# Device ì„ íƒ: cuda, mps, ì—†ìœ¼ë©´ cpu
if torch.cuda.is_available():
    device = "cuda"
elif torch.backends.mps.is_available():
    device = "mps"
else:
    device = "cpu"

# CLIP ëª¨ë¸ ë¡œë“œ (ViT-B/16)
clip_model, clip_preprocess = clip.load("ViT-B/16", device=device)
clip_model.eval()
transform = clip_preprocess  # CLIP ì „ì²˜ë¦¬ transform

# =====================
# í—¬í¼ í•¨ìˆ˜
# =====================

def _trim_css_to_bounds(css, image_shape):
    """ì´ë¯¸ì§€ ê²½ê³„ ë‚´ë¡œ ì¢Œí‘œ ì œí•œ (top, right, bottom, left)"""
    return (max(css[0], 0),
            min(css[1], image_shape[1]),
            min(css[2], image_shape[0]),
            max(css[3], 0))

def _rect_to_css(rect):
    """dlib rectë¥¼ CSS ìŠ¤íƒ€ì¼ ì¢Œí‘œ (top, right, bottom, left)ë¡œ ë³€í™˜"""
    return rect.top(), rect.right(), rect.bottom(), rect.left()

def _raw_face_locations(img, upsample_num=1):
    """ì–¼êµ´ ìœ„ì¹˜ ê²€ì¶œ"""
    return detector(img, upsample_num)

def face_locations(img, upsample_num=1):
    """ì–¼êµ´ ìœ„ì¹˜ ê²€ì¶œ ë° ì¢Œí‘œ ë³€í™˜"""
    detections = _raw_face_locations(img, upsample_num)
    return [_trim_css_to_bounds(_rect_to_css(face.rect), img.shape)
            for face in detections]

def extract_face_embedding(image, face_location, padding=50):
    """ì–¼êµ´ ì˜ì—­ì—ì„œ CLIP ì„ë² ë”© ì¶”ì¶œ"""
    top, right, bottom, left = face_location
    face_img = image[
        max(0, top - padding): min(image.shape[0], bottom + padding),
        max(0, left - padding): min(image.shape[1], right + padding)
    ]
    face_pil = Image.fromarray(cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB))
    face_tensor = transform(face_pil).unsqueeze(0).to(device)
    with torch.no_grad():
        embedding = clip_model.encode_image(face_tensor)
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°ì„ ìœ„í•´ ì •ê·œí™”
    embedding = embedding / embedding.norm(dim=-1, keepdim=True)
    return embedding

def extract_landmark_features(shape, image, gray_image):
    """ëœë“œë§ˆí¬ íŠ¹ì§• ì¶”ì¶œ (ì˜¤ë¥˜ ìƒí™©ì„ ê³ ë ¤í•˜ì—¬ robustí•˜ê²Œ ì²˜ë¦¬)"""
    coords = face_utils.shape_to_np(shape)
    features = []
    
    if coords.size == 0:
        return np.array(features)
    
    n_landmarks = coords.shape[0]
    
    # 1. ì–¼êµ´ ìœ¤ê³½ íŠ¹ì§•: aspect ratio (division by zero ë°©ì§€)
    face_width = np.max(coords[:, 0]) - np.min(coords[:, 0])
    face_height = np.max(coords[:, 1]) - np.min(coords[:, 1])
    aspect_ratio = face_width / face_height if face_height != 0 else 0
    features.append(aspect_ratio)
    
    # 2. ì–¼êµ´ ëŒ€ì¹­ì„±
    mid_x = np.mean(coords[:, 0])
    left_points = coords[coords[:, 0] < mid_x]
    right_points = coords[coords[:, 0] > mid_x]
    if left_points.size > 0 and right_points.size > 0:
        left_mean = np.mean(left_points, axis=0)
        right_mean = np.mean(right_points, axis=0)
        symmetry = np.linalg.norm(left_mean - right_mean)
        features.append(symmetry)
    
    # 3. ëˆˆ íŠ¹ì§• (ì „ì²´ ëœë“œë§ˆí¬ì˜ 1/3ì”©ì„ ëˆˆ ì˜ì—­ìœ¼ë¡œ ê°€ì •)
    third = n_landmarks // 3
    if third > 0:
        left_eye_points = coords[:third]
        right_eye_points = coords[third:2*third]
        if left_eye_points.size > 0 and right_eye_points.size > 0:
            left_eye_width = np.max(left_eye_points[:, 0]) - np.min(left_eye_points[:, 0])
            right_eye_width = np.max(right_eye_points[:, 0]) - np.min(right_eye_points[:, 0])
            eye_ratio = left_eye_width / right_eye_width if right_eye_width != 0 else 0
            features.append(eye_ratio)
            
            for eye_points in [left_eye_points, right_eye_points]:
                x1, y1 = np.min(eye_points, axis=0)
                x2, y2 = np.max(eye_points, axis=0)
                if x2 > x1 and y2 > y1:
                    eye_region = gray_image[y1:y2, x1:x2]
                    if eye_region.size > 0:
                        features.extend([
                            float(np.mean(eye_region)),
                            float(np.std(eye_region)),
                            float(np.max(eye_region) - np.min(eye_region))
                        ])
    
    # 4. ì½” íŠ¹ì§•: ì¶©ë¶„í•œ ëœë“œë§ˆí¬ê°€ ìˆì„ ë•Œë§Œ ì²˜ë¦¬
    if n_landmarks >= 18:
        nose_points = coords[12:18]
        if nose_points.size > 0:
            nose_width = np.max(nose_points[:, 0]) - np.min(nose_points[:, 0])
            nose_height = np.max(nose_points[:, 1]) - np.min(nose_points[:, 1])
            nose_ratio = nose_width / nose_height if nose_height != 0 else 0
            features.append(nose_ratio)
            
            x1, y1 = np.min(nose_points, axis=0)
            x2, y2 = np.max(nose_points, axis=0)
            if x2 > x1 and y2 > y1:
                nose_region = gray_image[y1:y2, x1:x2]
                if nose_region.size > 0:
                    features.extend([
                        float(np.mean(nose_region)),
                        float(np.std(nose_region)),
                        float(np.max(nose_region) - np.min(nose_region))
                    ])
    
    # 5. ìœ¤ê³½ì„  ê³¡ë¥ : ì¸ì ‘ ì ë“¤ ì‚¬ì´ì˜ ê°ë„ (0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²½ìš° ë°©ì§€)
    for i in range(1, n_landmarks - 1):
        p1, p2, p3 = coords[i - 1], coords[i], coords[i + 1]
        v1 = p1 - p2
        v2 = p3 - p2
        norm_product = np.linalg.norm(v1) * np.linalg.norm(v2)
        if norm_product > 0:
            angle = np.arccos(np.clip(np.dot(v1, v2) / norm_product, -1.0, 1.0))
            features.append(angle)
    
    # 6. í…ìŠ¤ì²˜ íŒ¨í„´: ê° ëœë“œë§ˆí¬ ì£¼ë³€ì˜ ê°„ë‹¨í•œ HOG íŠ¹ì§•
    patch_size = 7
    for (x, y) in coords.astype(int):
        x_start = max(0, x - patch_size)
        x_end = min(gray_image.shape[1], x + patch_size)
        y_start = max(0, y - patch_size)
        y_end = min(gray_image.shape[0], y + patch_size)
        patch = gray_image[y_start:y_end, x_start:x_end]
        if patch.size > 0:
            gx = cv2.Sobel(patch, cv2.CV_32F, 1, 0)
            gy = cv2.Sobel(patch, cv2.CV_32F, 0, 1)
            mag, ang = cv2.cartToPolar(gx, gy)
            features.extend([
                float(np.mean(mag)),
                float(np.std(mag)),
                float(np.mean(ang)),
                float(np.std(ang))
            ])
    
    # 7. ì»¬ëŸ¬ íŠ¹ì§•: ê° ëœë“œë§ˆí¬ ì£¼ë³€ì˜ BGR ì±„ë„ í‰ê· /í‘œì¤€í¸ì°¨
    for (x, y) in coords.astype(int):
        x_start = max(0, x - 3)
        x_end = min(image.shape[1], x + 3)
        y_start = max(0, y - 3)
        y_end = min(image.shape[0], y + 3)
        patch = image[y_start:y_end, x_start:x_end]
        if patch.size > 0:
            for i in range(3):
                features.extend([
                    float(np.mean(patch[:, :, i])),
                    float(np.std(patch[:, :, i]))
                ])
    
    return np.array(features)

def draw_annotations(image, face, shape, scores):
    """ì‹œê°í™”: ì–¼êµ´ ë°•ìŠ¤, ëœë“œë§ˆí¬, ì ìˆ˜ í‘œì‹œ"""
    top, right, bottom, left = face
    cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), 4)
    for (x, y) in face_utils.shape_to_np(shape):
        cv2.circle(image, (x, y), 3, (0, 0, 255), -1)
    text1 = f"Emb: {scores['embedding']:.2f}, Lmk: {scores['landmark']:.2f}"
    text2 = f"Combined: {scores['combined']:.2f}"
    cv2.putText(image, text1, (10, 30),
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
    cv2.putText(image, text2, (10, 60),
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

# =====================
# ì–¼êµ´ ë¹„êµ ë° ì‹œê°í™”
# =====================

def compare_faces(img1, img2, display=True):
    """
    ë‘ ì´ë¯¸ì§€ì˜ ì–¼êµ´(ê°•ì•„ì§€ ë¨¸ë¦¬)ì„ ë¹„êµí•˜ì—¬ ì„ë² ë”© ë° ëœë“œë§ˆí¬ ê¸°ë°˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    display=Trueì¸ ê²½ìš° matplotlibìœ¼ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.
    """
    gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
    gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
    faces1 = face_locations(gray1)
    faces2 = face_locations(gray2)
    
    if not faces1 or not faces2:
        print("í•˜ë‚˜ ì´ìƒì˜ ì´ë¯¸ì§€ì—ì„œ ê°•ì•„ì§€ ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None, None, None

    # ì²« ë²ˆì§¸ ê²€ì¶œëœ ì–¼êµ´ ì‚¬ìš©
    face1 = faces1[0]
    face2 = faces2[0]

    # dlib.rectangle ìƒì„± (dlib.rectangle(left, top, right, bottom))
    face1_rect = dlib.rectangle(face1[3], face1[0], face1[1], face1[2])
    face2_rect = dlib.rectangle(face2[3], face2[0], face2[1], face2[2])
    
    shape1 = predictor(gray1, face1_rect)
    shape2 = predictor(gray2, face2_rect)
    
    # ëœë“œë§ˆí¬ íŠ¹ì§• ì¶”ì¶œ (ì»¬ëŸ¬ì™€ ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ì´ë¯¸ì§€ ëª¨ë‘ ì‚¬ìš©)
    lmk_features1 = extract_landmark_features(shape1, img1, gray1)
    lmk_features2 = extract_landmark_features(shape2, img2, gray2)
    
    # CLIP ì„ë² ë”© ì¶”ì¶œ
    embedding1 = extract_face_embedding(img1, face1)
    embedding2 = extract_face_embedding(img2, face2)
    
    # ì„ë² ë”© ìœ ì‚¬ë„ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
    emb_sim = torch.nn.functional.cosine_similarity(embedding1, embedding2).item()
    
    # ëœë“œë§ˆí¬ íŠ¹ì§• ìœ ì‚¬ë„ (0 division ë°©ì§€)
    norm1 = np.linalg.norm(lmk_features1)
    norm2 = np.linalg.norm(lmk_features2)
    if norm1 + norm2 == 0:
        lmk_sim = 0
    else:
        lmk_sim = 1 - (np.linalg.norm(lmk_features1 - lmk_features2) / (norm1 + norm2))
    
    # ê°€ì¤‘ì¹˜ ì¡°ì • (ìƒí™©ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥)
    comb_score = 0.6 * emb_sim + 0.4 * lmk_sim

    result_img1 = img1.copy()
    result_img2 = img2.copy()
    scores = {'embedding': emb_sim, 'landmark': lmk_sim, 'combined': comb_score}
    
    draw_annotations(result_img1, face1, shape1, scores)
    draw_annotations(result_img2, face2, shape2, scores)
    
    # ì‹œê°í™”ë¥¼ ìœ„í•œ ì–¼êµ´ ì˜ì—­ ì¶”ì¶œ (íŒ¨ë”© ì ìš©)
    vis_padding = 50
    face1_img = img1[
        max(0, face1[0] - vis_padding):min(img1.shape[0], face1[2] + vis_padding),
        max(0, face1[3] - vis_padding):min(img1.shape[1], face1[1] + vis_padding)
    ]
    face2_img = img2[
        max(0, face2[0] - vis_padding):min(img2.shape[0], face2[2] + vis_padding),
        max(0, face2[3] - vis_padding):min(img2.shape[1], face2[1] + vis_padding)
    ]
    
    # display=Trueì¸ ê²½ìš°ì—ë§Œ matplotlib ì‹œê°í™” ì‹¤í–‰
    if display:
        plt.figure(figsize=(15, 8))
        plt.subplot(231)
        plt.imshow(cv2.cvtColor(result_img1, cv2.COLOR_BGR2RGB))
        plt.title('Image 1')
        plt.axis('off')
        
        plt.subplot(232)
        plt.imshow(cv2.cvtColor(result_img2, cv2.COLOR_BGR2RGB))
        plt.title('Image 2')
        plt.axis('off')
        
        plt.subplot(233)
        plt.text(0.5, 0.6, 'Similarity Scores:',
                 horizontalalignment='center',
                 verticalalignment='center',
                 fontsize=12, transform=plt.gca().transAxes)
        plt.text(0.5, 0.4,
                 f'Embedding: {emb_sim:.2f}\nLandmark: {lmk_sim:.2f}\nCombined: {comb_score:.2f}',
                 horizontalalignment='center',
                 verticalalignment='center',
                 fontsize=10, transform=plt.gca().transAxes)
        plt.axis('off')
        
        plt.subplot(235)
        plt.imshow(cv2.cvtColor(face1_img, cv2.COLOR_BGR2RGB))
        plt.title('Face 1')
        plt.axis('off')
        
        plt.subplot(236)
        plt.imshow(cv2.cvtColor(face2_img, cv2.COLOR_BGR2RGB))
        plt.title('Face 2')
        plt.axis('off')
        
        plt.tight_layout()
        plt.show()
    
    return result_img1, result_img2, comb_score

# =====================
# ë©”ì¸ ì‹¤í–‰ë¶€ (í…ŒìŠ¤íŠ¸ìš©)
# =====================
if __name__ == '__main__':
    img1_path = 'examples/dog5.jpg'
    img2_path = 'examples/dog5_1.jpg'
    
    img1 = cv2.imread(img1_path)
    img2 = cv2.imread(img2_path)
    
    if img1 is None or img2 is None:
        print("ì´ë¯¸ì§€ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        res1, res2, similarity = compare_faces(img1, img2, display=True)
        if res1 is not None:
            plt.figure(figsize=(15, 5))
            plt.subplot(121)
            plt.imshow(cv2.cvtColor(res1, cv2.COLOR_BGR2RGB))
            plt.title('Result Image 1')
            plt.axis('off')
            
            plt.subplot(122)
            plt.imshow(cv2.cvtColor(res2, cv2.COLOR_BGR2RGB))
            plt.title('Result Image 2')
            plt.axis('off')
            
            plt.suptitle(f'Combined Similarity: {similarity:.2f}', fontsize=16)
            plt.show()


def image_vector(img1):
    """
    ë‘ ì´ë¯¸ì§€ì˜ ì–¼êµ´(ê°•ì•„ì§€ ë¨¸ë¦¬)ì„ ë¹„êµí•˜ì—¬ ì„ë² ë”© ë° ëœë“œë§ˆí¬ ê¸°ë°˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    display=Trueì¸ ê²½ìš° matplotlibìœ¼ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.
    """
    gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
    faces1 = face_locations(gray1)

    if len(faces1) == 0:  # ğŸ›‘ ì–¼êµ´ì´ ê²€ì¶œë˜ì§€ ì•Šìœ¼ë©´ ì˜ˆì™¸ ì²˜ë¦¬ í•„ìš”
        raise ValueError("ì–¼êµ´ì„ ê°ì§€í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    face1 = faces1[0]

    # dlib.rectangle ìƒì„± (dlib.rectangle(left, top, right, bottom))
    face1_rect = dlib.rectangle(face1[3], face1[0], face1[1], face1[2])

    shape1 = predictor(gray1, face1_rect)

    # ëœë“œë§ˆí¬ íŠ¹ì§• ì¶”ì¶œ (ì»¬ëŸ¬ì™€ ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ì´ë¯¸ì§€ ëª¨ë‘ ì‚¬ìš©)
    lmk_features1 = extract_landmark_features(shape1, img1, gray1)

    # CLIP ì„ë² ë”© ì¶”ì¶œ
    embedding1 = extract_face_embedding(img1, face1)

    return lmk_features1, embedding1
